import time
import random
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è CONFIGURATION
# ==========================================
BASE_URL = "https://propertyhub.in.th"
START_PATH = "/%E0%B8%82%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%AD%E0%B8%99%E0%B9%82%E0%B8%94/"

# Path ‡πÉ‡∏ô Docker Container (Volume Map ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)
SAVE_DIR = "/opt/airflow/data/link"

# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á
DISTRICTS = [
    ('%E0%B8%88%E0%B8%95%E0%B8%B8%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%A3/', '‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%A7%E0%B8%A8/', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%A7%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2/', '‡∏ß‡∏±‡∏í‡∏ô‡∏≤'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B8%81%E0%B8%B0%E0%B8%9B%E0%B8%B4/', '‡∏ö‡∏≤‡∏á‡∏Å‡∏∞‡∏õ‡∏¥'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%84%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%95%E0%B8%A2/', '‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%81%E0%B8%84/', '‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9B%E0%B8%97%E0%B8%B8%E0%B8%A1%E0%B8%A7%E0%B8%B1%E0%B8%99/', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ß‡∏±‡∏ô'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%82%E0%B8%99/', '‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô'),
]

def get_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
    
    # --------------------------------------------------------
    # [‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ] ‡∏£‡∏∞‡∏ö‡∏∏ Path ‡∏Ç‡∏≠‡∏á Chromium ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏•‡∏á‡πÉ‡∏ô Docker
    # --------------------------------------------------------
    options.binary_location = "/usr/bin/chromium"
    
    # ‡πÉ‡∏ä‡πâ chromedriver ‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ú‡πà‡∏≤‡∏ô apt-get (‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà /usr/bin/chromedriver)
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ChromeDriverManager().install() ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏•‡∏á‡∏ú‡πà‡∏≤‡∏ô Dockerfile ‡πÅ‡∏•‡πâ‡∏ß
    service = Service("/usr/bin/chromedriver")
    
    return webdriver.Chrome(service=service, options=options)

def scrape_district(driver, district_path, district_name, max_pages=3):
    """‡∏î‡∏∂‡∏á Link ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    print(f"\n========== ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡∏ï: {district_name} ==========")
    
    seen_links = set()
    results = []

    for page in range(1, max_pages + 1):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á URL
        suffix = "" if page == 1 else f"{page}"
        url = urljoin(BASE_URL, START_PATH + district_path + suffix)
        
        print(f"   [Page {page}] Accessing: {url}")
        
        try:
            driver.get(url)
            time.sleep(random.uniform(1.5, 3.0)) # ‡∏£‡∏≠‡πÇ‡∏´‡∏•‡∏î
            
            # ‡∏´‡∏≤ Link ‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î (Update selector ‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏à‡∏£‡∏¥‡∏á)
            anchors = driver.find_elements(By.CSS_SELECTOR, "a.sc-152o12i-9.fhmSYQ") 
            
            if not anchors:
                print("   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Link ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ - ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏´‡∏°‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß")
                break
                
            count_new = 0
            for a in anchors:
                href = a.get_attribute("href")
                if href and href not in seen_links:
                    seen_links.add(href)
                    results.append(href)
                    count_new += 1
            
            print(f"   ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° {count_new} ‡∏•‡∏¥‡∏á‡∏Å‡πå (‡∏£‡∏ß‡∏° {len(results)})")
            
        except Exception as e:
            print(f"   ‚ùå Error on page {page}: {e}")
            break

    # Save to CSV
    os.makedirs(SAVE_DIR, exist_ok=True)
    out_path = os.path.join(SAVE_DIR, f"links_{district_name}.csv")
    
    df = pd.DataFrame({"url": results})
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {out_path}")

def main():
    print("üöÄ Starting Condo Link Scraper...")
    driver = get_driver()
    
    try:
        # Loop ‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ç‡∏ï
        # for d_path, d_name in DISTRICTS:
        #     # ‡∏õ‡∏£‡∏±‡∏ö max_pages ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ test)
        #     scrape_district(driver, d_path, d_name, max_pages=5) 
        scrape_district(driver, '%E0%B8%88%E0%B8%95%E0%B8%B8%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%A3/', '‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£', max_pages=5) 
    except Exception as e:
        print(f"üî• Critical Error: {e}")
    finally:
        driver.quit()
        print("üèÅ Scraper Finished.")

if __name__ == "__main__":
    main()