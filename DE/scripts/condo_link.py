import time
import random
import os
import sys
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è CONFIGURATION
# ==========================================
BASE_URL = "https://propertyhub.in.th"
START_PATH = "/%E0%B8%82%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%AD%E0%B8%99%E0%B9%82%E0%B8%94/"

# Path ‡πÉ‡∏ô Docker Container
SAVE_DIR = "/opt/airflow/data/link"

# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
DISTRICTS = [
    ('%E0%B8%88%E0%B8%95%E0%B8%B8%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%A3/', '‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£'),
    ('%E0%B9%80%E0%B8%82%E0%B8%95%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%A7%E0%B8%A8/', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®'),
    ('%E0%B8%82%E0%B8%95%E0%B8%A7%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2/', '‡∏ß‡∏±‡∏í‡∏ô‡∏≤'),
    ('%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B8%81%E0%B8%B0%E0%B8%9B%E0%B8%B4/', '‡∏ö‡∏≤‡∏á‡∏Å‡∏∞‡∏õ‡∏¥'),
    ('%E0%B8%82%E0%B8%95%E0%B8%84%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%95%E0%B8%A2/', '‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢'),
    ('%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%81%E0%B8%84/', '‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ'),
    ('%E0%B8%82%E0%B8%95%E0%B8%9B%E0%B8%97%E0%B8%B8%E0%B8%A1%E0%B8%A7%E0%B8%B1%E0%B8%99/', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ß‡∏±‡∏ô'),
    ('%E0%B8%82%E0%B8%95%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%82%E0%B8%99/', '‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô'),
]


def get_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0")

    options.binary_location = "/usr/bin/chromium"
    service = Service("/usr/bin/chromedriver")

    return webdriver.Chrome(service=service, options=options)


def scrape_district(driver, district_path, district_name, max_pages=3):
    print(f"\n========== ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡∏ï: {district_name} ==========")

    os.makedirs(SAVE_DIR, exist_ok=True)
    out_path = os.path.join(SAVE_DIR, f"links_{district_name}.csv")

    existing_links = set()
    file_exists = os.path.isfile(out_path)

    if file_exists:
        try:
            old_df = pd.read_csv(out_path)
            if "url" in old_df.columns:
                existing_links = set(old_df["url"].tolist())
            print(f"   üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°: ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß {len(existing_links)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        except Exception as e:
            print(f"   ‚ö†Ô∏è ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

    new_results = []

    for page in range(1, max_pages + 1):

        suffix = "" if page == 1 else f"{page}"
        url = urljoin(BASE_URL, START_PATH + district_path + suffix)

        print(f"   [Page {page}] Accessing: {url}")

        try:
            driver.get(url)
            time.sleep(random.uniform(1.5, 3.0))

            anchors = driver.find_elements(By.CSS_SELECTOR, "a.sc-152o12i-9.fhmSYQ")

            if not anchors:
                print("   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Link ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ - ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏´‡∏°‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß")
                break

            found_duplicate = False

            for a in anchors:
                href = a.get_attribute("href")
                if not href:
                    continue

                if href in existing_links:
                    print(f"   ‚ö†Ô∏è ‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏Å‡πà‡∏≤ ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤ {page} ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                    found_duplicate = True
                    break

                new_results.append(href)
                existing_links.add(href)

            if found_duplicate:
                continue

        except Exception as e:
            print(f"   ‚ùå Error on page {page}: {e}")
            break

    if new_results:
        df_new = pd.DataFrame({"url": new_results})
        mode = 'a' if file_exists else 'w'
        header = not file_exists

        df_new.to_csv(out_path, mode=mode, header=header, index=False, encoding="utf-8-sig")
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏° {len(new_results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà -> {out_path}")
    else:
        print("üí§ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ô‡∏µ‡πâ")


def main():
    print("üöÄ Starting Condo Link Scraper (Incremental Mode)...")
    driver = get_driver()

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö argument ‡∏à‡∏≤‡∏Å command line
    selected_district = None
    if len(sys.argv) > 1:
        selected_district = sys.argv[1].strip()

    try:
        # ‡∏ñ‡πâ‡∏≤ user ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï ‡πÄ‡∏ä‡πà‡∏ô ‚Äú‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£‚Äù
        if selected_district:
            print(f"üìå ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Ç‡∏ï: {selected_district}")

            found = False
            for d_path, d_name in DISTRICTS:
                if d_name == selected_district:
                    scrape_district(driver, d_path, d_name, max_pages=6)
                    found = True
                    break

            if not found:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏")
        else:
            # ‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏î‡∏¥‡∏°: ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ç‡∏ï
            for d_path, d_name in DISTRICTS:
                scrape_district(driver, d_path, d_name, max_pages=6)

    except Exception as e:
        print(f"üî• Critical Error: {e}")
    finally:
        driver.quit()
        print("üèÅ Scraper Finished.")


if __name__ == "__main__":
    main()
