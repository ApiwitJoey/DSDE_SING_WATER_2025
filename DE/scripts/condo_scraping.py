import time
import random
import pandas as pd
import re
import glob
import os
import sys  # <--- à¸ªà¸³à¸„à¸±à¸! à¹€à¸­à¸²à¹„à¸§à¹‰à¸£à¸±à¸šà¸Šà¸·à¹ˆà¸­à¹€à¸‚à¸•à¸ˆà¸²à¸ Airflow
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ==========================================
# âš™ï¸ CONFIGURATION
# ==========================================
INPUT_DIR = "/opt/airflow/data/link"
OUTPUT_DIR = "/opt/airflow/data/raw_condo"

# ==========================================
# ğŸ”§ Price Cleaner
# ==========================================
def clean_price_value(price_str):
    if not price_str: 
        return None
    try:
        clean = price_str.replace(",", "").replace("à¸šà¸²à¸—", "").strip()
        m = re.search(r"(\d+)", clean)
        return int(m.group(1)) if m else None
    except:
        return None

# ==========================================
# ğŸ§  Extract Details
# ==========================================
def extract_details(soup):
    details = {}
    
    # 1. Project Name
    h1 = soup.find("h1", class_="sc-rqf8dv-1 GAXpy")
    details["Project_Name"] = h1.get_text(strip=True) if h1 else None

    # 2. Condo Name (Secondary)
    condo = soup.find("span", class_="sc-ejnaz6-3 gSIBgi")
    details["condo_name"] = condo.get_text(strip=True) if condo else None

    # 3. Details List
    ul = soup.find("ul", class_="sc-ejnaz6-2 fuLHNZ")
    if ul:
        for li in ul.find_all("li"):
            label_tag = li.find("label")
            span_tag = li.find("span")
            if not label_tag or not span_tag:
                continue

            label = label_tag.get_text(strip=True)
            value = span_tag.get_text(strip=True)

            if label == "à¸£à¸²à¸„à¸²":
                details["Price"] = clean_price_value(value)
            elif label == "à¸£à¸¹à¸›à¹à¸šà¸šà¸«à¹‰à¸­à¸‡":
                details["Room_Type"] = value
            elif label == "à¸«à¹‰à¸­à¸‡à¸­à¸¢à¸¹à¹ˆà¸Šà¸±à¹‰à¸™à¸—à¸µà¹ˆ":
                details["Floor"] = value
            elif label == "à¸ˆà¸³à¸™à¸§à¸™à¸«à¹‰à¸­à¸‡à¸™à¸­à¸™":
                details["Bedrooms"] = value
            elif label == "à¸ˆà¸³à¸™à¸§à¸™à¸«à¹‰à¸­à¸‡à¸™à¹‰à¸³":
                details["Bathrooms"] = value
            elif label == "à¸‚à¸™à¸²à¸”à¸à¸·à¹‰à¸™à¸—à¸µà¹ˆà¸«à¹‰à¸­à¸‡":
                details["Room_Size"] = value

    return details

# ==========================================
# ğŸš— Selenium Driver (Chromium for Docker)
# ==========================================
def get_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0")

    # Path à¸ªà¸³à¸«à¸£à¸±à¸š Docker Image à¸—à¸µà¹ˆà¹€à¸£à¸²à¸¥à¸‡ Chromium à¹„à¸§à¹‰
    options.binary_location = "/usr/bin/chromium"
    service = Service("/usr/bin/chromedriver")

    return webdriver.Chrome(service=service, options=options)

# ==========================================
# ğŸ“ Scrape Function (Resume Safe)
# ==========================================
def scrape_one_district(district_name):
    print(f"\nğŸ“ Processing District: {district_name}")

    input_path = os.path.join(INPUT_DIR, f"links_{district_name}.csv")
    output_path = os.path.join(OUTPUT_DIR, f"condo_{district_name}.csv")

    if not os.path.exists(input_path):
        print(f"âš ï¸ No link file found: {input_path}")
        return

    df = pd.read_csv(input_path)

    # à¸«à¸² Column à¸—à¸µà¹ˆà¸Šà¸·à¹ˆà¸­à¸¡à¸µà¸„à¸³à¸§à¹ˆà¸² url à¸«à¸£à¸·à¸­ link
    link_col = next((c for c in df.columns if "url" in c.lower() or "link" in c.lower()), None)

    if not link_col:
        print("âŒ CSV format incorrect (no url column)")
        return

    # ---------------------------------------------------------
    # ğŸ§© à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸´à¸¡à¸–à¹‰à¸²à¸¡à¸µ à¹€à¸à¸·à¹ˆà¸­à¸™à¸³à¸¡à¸² skip + append
    # ---------------------------------------------------------
    already_scraped = set()
    if os.path.exists(output_path):
        old_df = pd.read_csv(output_path)
        if "Original_Link" in old_df.columns:
            already_scraped = set(old_df["Original_Link"])
        print(f"ğŸ“„ Loaded existing file with {len(already_scraped)} previous rows")
    else:
        old_df = pd.DataFrame()

    driver = get_driver()
    wait = WebDriverWait(driver, 10)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    new_rows = []
    total = len(df)

    for idx, row in df.iterrows():
        url = row[link_col]

        # Skip URL à¸—à¸µà¹ˆà¹€à¸„à¸¢ scrape à¹à¸¥à¹‰à¸§
        if url in already_scraped:
            print(f"   [{idx+1}/{total}] â© Skip (already scraped): {url}")
            continue

        print(f"   [{idx+1}/{total}] Scraping: {url}")

        try:
            driver.get(url)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            soup = BeautifulSoup(driver.page_source, "html.parser")

            info = extract_details(soup)
            info["Original_Link"] = url

            print(f"      ---> Price: {info.get('Price', 'N/A')}")
            new_rows.append(info)

            # Backup à¸—à¸¸à¸ 10 à¸£à¸²à¸¢à¸à¸²à¸£
            if len(new_rows) % 10 == 0:
                combined = pd.concat([old_df, pd.DataFrame(new_rows)], ignore_index=True)
                combined.to_csv(output_path, index=False, encoding="utf-8-sig")
                print("      ğŸ’¾ Backup Saved.")

        except Exception as e:
            print(f"      âŒ Error: {e}")

    driver.quit()

    # ---------------------------------------------------------
    # ğŸ”¥ Save Final = append old + new
    # ---------------------------------------------------------
    if new_rows:
        combined = pd.concat([old_df, pd.DataFrame(new_rows)], ignore_index=True)
        combined.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… Finished {district_name}: {len(new_rows)} new rows saved.")
    else:
        print(f"âš ï¸ No new data for {district_name} (all URLs already scraped)")

# ==========================================
# ğŸš€ MAIN (Support Airflow Argument)
# ==========================================
def main():
    print("ğŸš€ Starting Condo Details Scraper...")

    # à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² Airflow à¸ªà¹ˆà¸‡ argument à¹€à¸‚à¸•à¸¡à¸²à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¹ˆà¸²
    if len(sys.argv) > 1:
        target_district = sys.argv[1]
        print(f"ğŸ¯ Targeted Mode: Scraping ONLY '{target_district}'")
        scrape_one_district(target_district)

    else:
        # Batch mode - à¸—à¸³à¸—à¸¸à¸à¹€à¸‚à¸•à¸—à¸µà¹ˆà¸¡à¸µà¹„à¸Ÿà¸¥à¹Œ
        print("ğŸ”„ Batch Mode: Scraping ALL found link files...")
        files = glob.glob(os.path.join(INPUT_DIR, "links_*.csv"))

        if not files:
            print(f"âŒ No link files found in {INPUT_DIR}")
            return

        districts = [
            os.path.basename(f).replace("links_", "").replace(".csv", "")
            for f in files
        ]

        for d in districts:
            scrape_one_district(d)

    print("ğŸ Script Finished.")

if __name__ == "__main__":
    main()
